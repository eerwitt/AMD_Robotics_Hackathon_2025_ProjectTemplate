**### Make a fork or copy of this repo and fill in your team submission details! ###**

# AMD_Robotics_Hackathon_2025_XVLA_Beats

## Team Information

**Team:** 24, Sigmoid, Erik Erwitt

**Summary:** Fine tuned XVLA based on MEL and Chroma spectrographs of MP3 files with MP3s split up into different episodes.

*< Images or video demonstrating your project >*

## Submission Details

### 1. Mission Description
- Everyone likes to dance, even robots. More importantly it is to understand if I may be able to tune XVLA off more dynamic data and environment situations.

### 2. Creativity
- Using audio for the image input.

### 3. Technical implementations
- *Teleoperation / Dataset capture*
    - This can be recorded with test-beats.py
    - *<Image/video of teleoperation or dataset capture>*
- *Training*
    - lab/tree/training-models-on-rocm.ipynb
```py
lerobot-train \
  --dataset.repo_id="eerwitt/xvla-beats-run009" \
  --output_dir=./outputs/xvla_training-12 \
  --job_name=xvla_training \
  --policy.path="lerobot/xvla-base" \
  --policy.repo_id="eerwitt/xvla-beats-2" \
  --policy.dtype=bfloat16 \
  --steps=3000 \
  --policy.device=cuda \
  --policy.freeze_vision_encoder=false \
  --policy.freeze_language_encoder=false \
  --policy.train_policy_transformer=true \
  --policy.train_soft_prompts=true \
  --policy.action_mode=auto \
  --rename_map='{"observation.images.chroma": "observation.images.image", "observation.images.mel": "observation.images.image2"}'
  ```

- *Inference*
    - This video is faked, will update with real but ran into update due to imports with torchvision on latest xvla. Cannot replay since it needs the custom video of MEL and Chroma for audio to video.
    - Working model: https://huggingface.co/eerwitt/xvla-beats-2 (only trained off 2 songs split into many episodes)

### 4. Ease of use
- Very hard :)

## Additional Links
*For example, you can provide links to:*

- *Link to a video of your robot performing the task*
- *URL of your dataset in Hugging Face*
- *URL of your model in Hugging Face*
- *Link to a blog post describing your work*

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
